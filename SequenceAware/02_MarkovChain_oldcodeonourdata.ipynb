{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and build matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/archnnj/Development/recsys/recsys_polimi_challenge_2018/repo\n"
     ]
    }
   ],
   "source": [
    "cd ../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline\n",
    "import scipy.sparse as sps\n",
    "from scipy.stats import iqr\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":12,\"axes.titlesize\":12,\"axes.labelsize\":12})\n",
    "\n",
    "import src.utils.build_icm as build_icm\n",
    "from src.utils.data_splitter import train_test_holdout, train_test_user_holdout, train_test_row_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src/libs/RecSys_Course_2018/\") # go to parent dir\n",
    "sys.path.append(\"src/libs/RecSys_Course_2018/SequenceAware/sars_tutorial_master/\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SequenceAware.sars_tutorial_master.util.data_utils import create_seq_db_filter_top_k, sequences_to_spfm_format\n",
    "from SequenceAware.sars_tutorial_master.util.data_utils import create_seq_db_filter_top_k, sequences_to_spfm_format\n",
    "from SequenceAware.sars_tutorial_master.util.split import last_session_out_split\n",
    "from SequenceAware.sars_tutorial_master.util.metrics import precision, recall, mrr\n",
    "from SequenceAware.sars_tutorial_master.util import evaluation\n",
    "from SequenceAware.sars_tutorial_master.recommenders_original.MixedMarkovRecommender import MixedMarkovChainRecommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUPYTER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if JUPYTER:\n",
    "    # Jupyter\n",
    "    tracks_csv_file = \"../../../data/tracks.csv\"\n",
    "    interactions_csv_file = \"../../../data/train.csv\"\n",
    "    playlist_id_csv_file = \"../../../data/target_playlists.csv\"\n",
    "    sequential_csv_file = \"../../../data/train_sequential.csv\"\n",
    "else:\n",
    "    # PyCharm\n",
    "    tracks_csv_file = \"data/tracks.csv\"\n",
    "    interactions_csv_file = \"data/train.csv\"\n",
    "    playlist_id_csv_file = \"data/target_playlists.csv\"\n",
    "    sequential_csv_file = \"data/train_sequential.csv\"\n",
    "\n",
    "tracks_df = pd.read_csv(tracks_csv_file)\n",
    "interactions_df = pd.read_csv(interactions_csv_file)\n",
    "playlist_id_df = pd.read_csv(playlist_id_csv_file)\n",
    "train_sequential_df = pd.read_csv(sequential_csv_file)\n",
    "\n",
    "userList = interactions_df[\"playlist_id\"]\n",
    "itemList = interactions_df[\"track_id\"]\n",
    "ratingList = np.ones(interactions_df.shape[0])\n",
    "targetsList = playlist_id_df[\"playlist_id\"]\n",
    "targetsListOrdered = targetsList[:5000].tolist()\n",
    "targetsListCasual = targetsList[5000:].tolist()\n",
    "\n",
    "userList_unique = pd.unique(userList)\n",
    "itemList_unique = tracks_df[\"track_id\"]\n",
    "numUsers = len(userList_unique)\n",
    "numItems = len(itemList_unique)\n",
    "numberInteractions = interactions_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all_csr = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPopularity = (URM_all>0).sum(axis=0)\n",
    "itemPopularity = np.array(itemPopularity).squeeze()\n",
    "itemPopularity_unsorted = itemPopularity\n",
    "itemPopularity = np.sort(itemPopularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare ICM and URM with splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build ICM\n",
    "ICM_all = build_icm.build_icm(tracks_df, split_duration_lenght=800, feature_weights={'albums': 1, 'artists': 0.5, 'durations': 0.1})\n",
    "\n",
    "IDF_ENABLED = True\n",
    "\n",
    "if IDF_ENABLED:\n",
    "    num_tot_items = ICM_all.shape[0]\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "    ICM_idf = ICM_all.copy()\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(sps.csc_matrix(ICM_idf).indptr)\n",
    "    # then normalize the values in each col\n",
    "    ICM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    ICM_all = ICM_idf  # use IDF features\n",
    "\n",
    "# #### Build URM\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all_csr = URM_all.tocsr()\n",
    "\n",
    "URM_IDF_ENABLED = False\n",
    "\n",
    "if URM_IDF_ENABLED:\n",
    "    num_tot_items = URM_all.shape[0]\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (URM_all > 0).sum(axis=0)\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "    URM_idf = URM_all.copy()\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(sps.csc_matrix(URM_idf).indptr)\n",
    "    # then normalize the values in each col\n",
    "    URM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    URM_all = URM_idf  # use IDF features\n",
    "\n",
    "# #### Train/test split: ratings and user holdout\n",
    "\n",
    "seed = 0\n",
    "# ratings holdout\n",
    "# URM_train, URM_test_pred = train_test_holdout(URM_all, train_perc=0.8, seed=seed)\n",
    "# URM_valid=URM_test_pred\n",
    "# URM_test_known = None\n",
    "\n",
    "# user holdout\n",
    "# URM_train, URM_test_known, URM_test_pred = train_test_user_holdout(URM_all, user_perc=0.8, train_perc=0.8, seed=seed)\n",
    "\n",
    "# row holdout\n",
    "URM_train, URM_test_pred = train_test_row_holdout(URM_all, userList_unique, train_sequential_df, train_perc=0.8, seed=seed, targetsListOrdered=targetsListOrdered, nnz_threshold=10)\n",
    "URM_test_known = None\n",
    "\n",
    "# row holdout - validation\n",
    "# URM_train_val, URM_test_pred = train_test_row_holdout(URM_all, userList_unique, train_sequential_df, train_perc=0.8,\n",
    "#                                                       seed=seed, targetsListOrdered=targetsListOrdered,\n",
    "#                                                       nnz_threshold=10)\n",
    "# URM_train, URM_valid = train_test_holdout(URM_train_val, train_perc=0.7, seed=seed)\n",
    "# URM_test_known = None\n",
    "# URM_train, URM_valid_test_pred = train_test_row_holdout(URM_all, userList_unique, train_sequential_df,\n",
    "#                                                        train_perc=0.6,\n",
    "#                                                        seed=seed, targetsListOrdered=targetsListOrdered,\n",
    "#                                                        nnz_threshold=2)\n",
    "# URM_valid, URM_test_pred = train_test_row_holdout(URM_valid_test_pred, userList_unique, train_sequential_df,\n",
    "#                                                   train_perc=0.5,\n",
    "#                                                  seed=seed, targetsListOrdered=targetsListOrdered,\n",
    "#                                                  nnz_threshold=1)\n",
    "\n",
    "URM_train = URM_train\n",
    "#URM_validation = URM_valid\n",
    "URM_test = URM_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_df(URM, train_sequential_df, user_arr):\n",
    "    sequences_arr = []\n",
    "    for user_id in user_arr:\n",
    "        URM_start_pos = URM.indptr[user_id]\n",
    "        URM_end_pos = URM.indptr[user_id + 1]\n",
    "        user_nnz = URM_end_pos - URM_start_pos\n",
    "        \n",
    "        if user_nnz > 0:\n",
    "            original_order_items = np.array(\n",
    "                train_sequential_df.loc[train_sequential_df['playlist_id'].isin([user_id])]['track_id'])\n",
    "            original_order_items = np.array([i for i in original_order_items if i in URM.indices[URM_start_pos:URM_end_pos]]) # filter items in urm\n",
    "            # original_order_items_sorted_i = original_order_items.argsort()\n",
    "            sequences_arr.append({'user_id':user_id, 'sequence': original_order_items.tolist()})\n",
    "    sequences = pd.DataFrame(sequences_arr)\n",
    "    sequences = sequences[['user_id', 'sequence']]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = get_sequences_df(URM_all.tocsr(), train_sequential_df, targetsListOrdered)\n",
    "train_sequences = get_sequences_df(URM_train, train_sequential_df, targetsListOrdered)\n",
    "test_sequences = get_sequences_df(URM_test, train_sequential_df, targetsListOrdered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>[12493, 17495, 13424, 7109, 14714, 15650, 1574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>[13805, 7035, 2336, 16663, 4720, 19967, 12384,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>[17056, 16912, 14907, 4148, 15430, 4543, 16387...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>[16759, 9266, 13302, 18536, 8248, 523, 11656, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>[14417, 19813, 5281, 6245, 2388, 13452, 153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                           sequence\n",
       "0        7  [12493, 17495, 13424, 7109, 14714, 15650, 1574...\n",
       "1       25  [13805, 7035, 2336, 16663, 4720, 19967, 12384,...\n",
       "2       29  [17056, 16912, 14907, 4148, 15430, 4543, 16387...\n",
       "3       34  [16759, 9266, 13302, 18536, 8248, 523, 11656, ...\n",
       "4       50  [14417, 19813, 5281, 6245, 2388, 13452, 153, 1..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>[12493, 17495, 13424, 7109, 14714, 15650, 1574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>[13805, 7035, 2336, 16663, 4720, 19967, 12384,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>[17056, 16912, 14907, 4148, 15430, 4543, 16387...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>[16759, 9266, 13302, 18536, 8248, 523, 11656, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>[14417, 19813, 5281, 6245, 2388, 13452, 153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                           sequence\n",
       "0        7  [12493, 17495, 13424, 7109, 14714, 15650, 1574...\n",
       "1       25  [13805, 7035, 2336, 16663, 4720, 19967, 12384,...\n",
       "2       29  [17056, 16912, 14907, 4148, 15430, 4543, 16387...\n",
       "3       34  [16759, 9266, 13302, 18536, 8248, 523, 11656, ...\n",
       "4       50  [14417, 19813, 5281, 6245, 2388, 13452, 153, 1..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>[8869, 13359, 15296, 17114, 3767, 5668, 15640]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>[13637, 4910, 89]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>[9003, 6606, 18669, 18904, 13364]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>[3753, 6707, 19541, 10992, 18661, 14579]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>[10496, 13757]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                        sequence\n",
       "0        7  [8869, 13359, 15296, 17114, 3767, 5668, 15640]\n",
       "1       25                               [13637, 4910, 89]\n",
       "2       29               [9003, 6606, 18669, 18904, 13364]\n",
       "3       34        [3753, 6707, 19541, 10992, 18661, 14579]\n",
       "4       50                                  [10496, 13757]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start of original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>[12493, 17495, 13424, 7109, 14714, 15650, 1574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>[13805, 7035, 2336, 16663, 4720, 19967, 12384,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>[17056, 16912, 14907, 4148, 15430, 4543, 16387...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>[16759, 9266, 13302, 18536, 8248, 523, 11656, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>[14417, 19813, 5281, 6245, 2388, 13452, 153, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                           sequence\n",
       "0        7  [12493, 17495, 13424, 7109, 14714, 15650, 1574...\n",
       "1       25  [13805, 7035, 2336, 16663, 4720, 19967, 12384,...\n",
       "2       29  [17056, 16912, 14907, 4148, 15430, 4543, 16387...\n",
       "3       34  [16759, 9266, 13302, 18536, 8248, 523, 11656, ...\n",
       "4       50  [14417, 19813, 5281, 6245, 2388, 13452, 153, 1..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "dataset.sequence.map(cnt.update);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 13536\n",
      "Number of users: 5000\n",
      "Number of sessions: 5000\n",
      "\n",
      "Session length:\n",
      "\tAverage: 23.11\n",
      "\tMedian: 20.0\n",
      "\tMin: 7\n",
      "\tMax: 75\n",
      "Sessions per user:\n",
      "\tAverage: 1.00\n",
      "\tMedian: 1.0\n",
      "\tMin: 1\n",
      "\tMax: 1\n"
     ]
    }
   ],
   "source": [
    "sequence_length = dataset.sequence.map(len).values\n",
    "n_sessions_per_user = dataset.groupby('user_id').size()\n",
    "\n",
    "print('Number of items: {}'.format(len(cnt)))\n",
    "print('Number of users: {}'.format(dataset.user_id.nunique()))\n",
    "print('Number of sessions: {}'.format(len(dataset)) )\n",
    "\n",
    "print('\\nSession length:\\n\\tAverage: {:.2f}\\n\\tMedian: {}\\n\\tMin: {}\\n\\tMax: {}'.format(\n",
    "    sequence_length.mean(), \n",
    "    np.quantile(sequence_length, 0.5), \n",
    "    sequence_length.min(), \n",
    "    sequence_length.max()))\n",
    "\n",
    "print('Sessions per user:\\n\\tAverage: {:.2f}\\n\\tMedian: {}\\n\\tMin: {}\\n\\tMax: {}'.format(\n",
    "    n_sessions_per_user.mean(), \n",
    "    np.quantile(n_sessions_per_user, 0.5), \n",
    "    n_sessions_per_user.min(), \n",
    "    n_sessions_per_user.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular items: [(5606, 181), (10848, 160), (8956, 157), (15578, 153), (12393, 150)]\n"
     ]
    }
   ],
   "source": [
    "print('Most popular items: {}'.format(cnt.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split_the_dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's split the dataset by assigning the **last session** of every user to the **test set**, and **all the previous** ones to the **training set**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data, test_data = last_session_out_split(dataset)\n",
    "print(\"Train sessions: {} - Test sessions: {}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_sequences\n",
    "test_data = test_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fitting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fitting the recommender\n",
    "\n",
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "This recommender is based on the `MarkovChainRecommender` implemented from:\n",
    "\n",
    "_Shani, Guy, David Heckerman, and Ronen I. Brafman. \"An MDP-based recommender system.\" Journal of Machine Learning Research 6, no. Sep (2005): 1265-1295. Chapter 3-4_\n",
    "\n",
    "This recommender computes the item transition matrices for any Markov Chain having order in `[min_order, max_order]`. Each individual Markov Chain model employes some heristics like skipping or clustering to deal better with data sparsity. Recommendations are generated by sorting items by their transition probability to being next, given the user profile. The scores coming from different MC are weighted _inversely_ wrt to their order.\n",
    "\n",
    "The class `MixedMarkovChainRecommender` has the following initialization hyper-parameters:\n",
    "* `min_order`: the minimum order of the Mixed Markov Chain\n",
    "* `max_order`: the maximum order of the Mixed Markov Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try with max_order=2 or higher too, but it will take some time to complete though due to slow heristic computations\n",
    "recommender = MixedMarkovChainRecommender(min_order=1, \n",
    "                                          max_order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-03 03:16:09,389 - INFO - Building Markov Chain model with k = 1\n",
      "2019-01-03 03:16:09,390 - INFO - Adding nodes\n",
      "2019-01-03 03:16:14,852 - INFO - Adding edges\n",
      "2019-01-03 03:31:47,762 - INFO - Applying skipping\n",
      "2019-01-03 03:31:52,920 - INFO - Applying clustering\n",
      "2019-01-03 03:31:52,921 - INFO - 12540 states in the graph\n"
     ]
    }
   ],
   "source": [
    "recommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='seq_evaluation'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequential evaluation\n",
    "\n",
    "In the evaluation of sequence-aware recommenders, each sequence in the test set is split into:\n",
    "- the _user profile_, used to compute recommendations, is composed by the first *k* events in the sequence;\n",
    "- the _ground truth_, used for performance evaluation, is composed by the remainder of the sequence.\n",
    "\n",
    "In the cells below, you can control the dimension of the _user profile_ by assigning a **positive** value to `GIVEN_K`, which correspond to the number of events from the beginning of the sequence that will be assigned to the initial user profile. This ensures that each user profile in the test set will have exactly the same initial size, but the size of the ground truth will change for every sequence.\n",
    "\n",
    "Alternatively, by assigning a **negative** value to `GIVEN_K`, you will set the initial size of the _ground truth_. In this way the _ground truth_ will have the same size for all sequences, but the dimension of the user profile will differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {'precision':precision, \n",
    "           'recall':recall,\n",
    "           'mrr': mrr}\n",
    "TOPN = 10 # length of the recommendation list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval_seq_rev'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluation with sequentially revealed user-profiles\n",
    "\n",
    "Here we evaluate the quality of the recommendations in a setting in which user profiles are revealed _sequentially_.\n",
    "\n",
    "The _user profile_ starts from the first `GIVEN_K` events (or, alternatively, from the last `-GIVEN_K` events if `GIVEN_K<0`).  \n",
    "The recommendations are evaluated against the next `LOOK_AHEAD` events (the _ground truth_).  \n",
    "The _user profile_ is next expanded to the next `STEP` events, the ground truth is scrolled forward accordingly, and the evaluation continues until the sequence ends.\n",
    "\n",
    "In typical **next-item recommendation**, we start with `GIVEN_K=1`, generate a set of **alternatives** that will evaluated against the next event in the sequence (`LOOK_AHEAD=1`), move forward of one step (`STEP=1`) and repeat until the sequence ends.\n",
    "\n",
    "You can set the `LOOK_AHEAD='all'` to see what happens if you had to recommend a **whole sequence** instead of a set of a set of alternatives to a user.\n",
    "\n",
    "NOTE: Metrics are averaged over each sequence first, then averaged over all test sequences.\n",
    "\n",
    "** (TODO) Try out with different evaluation settings to see how the recommandation quality changes. **\n",
    "\n",
    "\n",
    "![](gifs/sequential_eval.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIVEN_K=1, LOOK_AHEAD=1, STEP=1 corresponds to the classical next-item evaluation\n",
    "GIVEN_K = 1\n",
    "LOOK_AHEAD = 1\n",
    "STEP=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4465 [00:00<00:04, 1105.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4465 sequences available for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "\n",
    "results = evaluation.sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=True,  # scrolling averages metrics over all profile lengths\n",
    "                                           step=STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sequential evaluation (GIVEN_K={}, LOOK_AHEAD={}, STEP={})'.format(GIVEN_K, LOOK_AHEAD, STEP))\n",
    "for mname, mvalue in zip(METRICS.keys(), results):\n",
    "    print('\\t{}@{}: {:.4f}'.format(mname, TOPN, mvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval_static'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation with \"static\" user-profiles\n",
    "\n",
    "Here we evaluate the quality of the recommendations in a setting in which user profiles are instead _static_.\n",
    "\n",
    "The _user profile_ starts from the first `GIVEN_K` events (or, alternatively, from the last `-GIVEN_K` events if `GIVEN_K<0`).  \n",
    "The recommendations are evaluated against the next `LOOK_AHEAD` events (the _ground truth_).  \n",
    "\n",
    "The user profile is *not extended* and the ground truth *doesn't move forward*.\n",
    "This allows to obtain \"snapshots\" of the recommendation performance for different user profile and ground truth lenghts.\n",
    "\n",
    "Also here you can set the `LOOK_AHEAD='all'` to see what happens if you had to recommend a **whole sequence** instead of a set of a set of alternatives to a user.\n",
    "\n",
    "**(TODO) Try out with different evaluation settings to see how the recommandation quality changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIVEN_K = 1\n",
    "LOOK_AHEAD = 'all'\n",
    "STEP=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "\n",
    "results = evaluation.sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=False  # notice that scrolling is disabled!\n",
    "                                          )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sequential evaluation (GIVEN_K={}, LOOK_AHEAD={}, STEP={})'.format(GIVEN_K, LOOK_AHEAD, STEP))\n",
    "for mname, mvalue in zip(METRICS.keys(), results):\n",
    "    print('\\t{}@{}: {:.4f}'.format(mname, TOPN, mvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next-item'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of next-item recommendation\n",
    "\n",
    "Here we propose to analyse the performance of the recommender system in the scenario of *next-item recommendation* over the following dimensions:\n",
    "\n",
    "* the *length* of the **recommendation list**, and\n",
    "* the *length* of the **user profile**.\n",
    "\n",
    "NOTE: This evaluation is by no means exhaustive, as different the hyper-parameters of the recommendation algorithm should be *carefully tuned* before drawing any conclusions. Unfortunately, given the time constraints for this tutorial, we had to leave hyper-parameter tuning out. A very useful reference about careful evaluation of (session-based) recommenders can be found at:\n",
    "\n",
    "*  Evaluation of Session-based Recommendation Algorithms, Ludewig and Jannach, 2018 ([paper](https://arxiv.org/abs/1803.09587))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next-item_list_length'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluation for different recommendation list lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIVEN_K = 1\n",
    "LOOK_AHEAD = 1\n",
    "STEP = 1\n",
    "topn_list = [1, 5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that all sequences have the same minimum length \n",
    "test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "\n",
    "for topn in topn_list:\n",
    "    print('Evaluating recommendation lists with length: {}'.format(topn))\n",
    "    res_tmp = evaluation.sequential_evaluation(recommender,\n",
    "                                               test_sequences=test_sequences,\n",
    "                                               given_k=GIVEN_K,\n",
    "                                               look_ahead=LOOK_AHEAD,\n",
    "                                               evaluation_functions=METRICS.values(),\n",
    "                                               top_n=topn,\n",
    "                                               scroll=True,  # here we average over all profile lengths\n",
    "                                               step=STEP)\n",
    "    mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "    res_list.append((topn, mvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show separate plots per metric\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5))\n",
    "res_list_t = list(zip(*res_list))\n",
    "for midx, metric in enumerate(METRICS):\n",
    "    mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))]\n",
    "    ax = axes[midx]\n",
    "    ax.plot(topn_list, mvalues)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticks(topn_list)\n",
    "    ax.set_xlabel('List length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next-item_profile_length'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation for different user profile lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_k_list = [1, 2, 3, 4]\n",
    "LOOK_AHEAD = 1\n",
    "STEP = 1\n",
    "TOPN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that all sequences have the same minimum length \n",
    "test_sequences = get_test_sequences(test_data, max(given_k_list))\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "\n",
    "for gk in given_k_list:\n",
    "    print('Evaluating profiles having length: {}'.format(gk))\n",
    "    res_tmp = evaluation.sequential_evaluation(recommender,\n",
    "                                               test_sequences=test_sequences,\n",
    "                                               given_k=gk,\n",
    "                                               look_ahead=LOOK_AHEAD,\n",
    "                                               evaluation_functions=METRICS.values(),\n",
    "                                               top_n=TOPN,\n",
    "                                               scroll=False,  # here we stop at each profile length\n",
    "                                               step=STEP)\n",
    "    mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "    res_list.append((gk, mvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show separate plots per metric\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5))\n",
    "res_list_t = list(zip(*res_list))\n",
    "for midx, metric in enumerate(METRICS):\n",
    "    mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))]\n",
    "    ax = axes[midx]\n",
    "    ax.plot(given_k_list, mvalues)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticks(given_k_list)\n",
    "    ax.set_xlabel('Profile length')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
