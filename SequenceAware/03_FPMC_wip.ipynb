{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and build matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/archnnj/Development/recsys/recsys_polimi_challenge_2018/repo\n"
     ]
    }
   ],
   "source": [
    "cd ../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline\n",
    "import scipy.sparse as sps\n",
    "from scipy.stats import iqr\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":12,\"axes.titlesize\":12,\"axes.labelsize\":12})\n",
    "\n",
    "import src.utils.build_icm as build_icm\n",
    "from src.utils.data_splitter import train_test_holdout, train_test_user_holdout, train_test_row_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src/libs/RecSys_Course_2018/\") # go to parent dir\n",
    "#sys.path.append(\"src/libs/RecSys_Course_2018/SequenceAware/sars_tutorial_master/\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SequenceAware.sars_tutorial_master.util.data_utils import create_seq_db_filter_top_k, sequences_to_spfm_format\n",
    "from SequenceAware.sars_tutorial_master.util.split import last_session_out_split\n",
    "from SequenceAware.sars_tutorial_master.util.metrics import precision, recall, mrr\n",
    "from SequenceAware.sars_tutorial_master.util import evaluation\n",
    "from SequenceAware.sars_tutorial_master.recommenders.FPMCRecommender import FPMCRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUPYTER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if JUPYTER:\n",
    "    # Jupyter\n",
    "    tracks_csv_file = \"../../../data/tracks.csv\"\n",
    "    interactions_csv_file = \"../../../data/train.csv\"\n",
    "    playlist_id_csv_file = \"../../../data/target_playlists.csv\"\n",
    "    sequential_csv_file = \"../../../data/train_sequential.csv\"\n",
    "else:\n",
    "    # PyCharm\n",
    "    tracks_csv_file = \"data/tracks.csv\"\n",
    "    interactions_csv_file = \"data/train.csv\"\n",
    "    playlist_id_csv_file = \"data/target_playlists.csv\"\n",
    "    sequential_csv_file = \"data/train_sequential.csv\"\n",
    "\n",
    "tracks_df = pd.read_csv(tracks_csv_file)\n",
    "interactions_df = pd.read_csv(interactions_csv_file)\n",
    "playlist_id_df = pd.read_csv(playlist_id_csv_file)\n",
    "train_sequential_df = pd.read_csv(sequential_csv_file)\n",
    "\n",
    "userList = interactions_df[\"playlist_id\"]\n",
    "itemList = interactions_df[\"track_id\"]\n",
    "ratingList = np.ones(interactions_df.shape[0])\n",
    "targetsList = playlist_id_df[\"playlist_id\"]\n",
    "targetsListOrdered = targetsList[:5000].tolist()\n",
    "targetsListCasual = targetsList[5000:].tolist()\n",
    "\n",
    "userList_unique = pd.unique(userList)\n",
    "itemList_unique = tracks_df[\"track_id\"]\n",
    "numUsers = len(userList_unique)\n",
    "numItems = len(itemList_unique)\n",
    "numberInteractions = interactions_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all_csr = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPopularity = (URM_all>0).sum(axis=0)\n",
    "itemPopularity = np.array(itemPopularity).squeeze()\n",
    "itemPopularity_unsorted = itemPopularity\n",
    "itemPopularity = np.sort(itemPopularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare ICM and URM with splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build ICM\n",
    "ICM_all = build_icm.build_icm(tracks_df, split_duration_lenght=800, feature_weights={'albums': 1, 'artists': 0.5, 'durations': 0.1})\n",
    "\n",
    "IDF_ENABLED = True\n",
    "\n",
    "if IDF_ENABLED:\n",
    "    num_tot_items = ICM_all.shape[0]\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (ICM_all > 0).sum(axis=0)\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "    ICM_idf = ICM_all.copy()\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(sps.csc_matrix(ICM_idf).indptr)\n",
    "    # then normalize the values in each col\n",
    "    ICM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    ICM_all = ICM_idf  # use IDF features\n",
    "\n",
    "# #### Build URM\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all_csr = URM_all.tocsr()\n",
    "\n",
    "URM_IDF_ENABLED = False\n",
    "\n",
    "if URM_IDF_ENABLED:\n",
    "    num_tot_items = URM_all.shape[0]\n",
    "    # let's count how many items have a certain feature\n",
    "    items_per_feature = (URM_all > 0).sum(axis=0)\n",
    "    IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "    URM_idf = URM_all.copy()\n",
    "    # compute the number of non-zeros in each col\n",
    "    # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "    col_nnz = np.diff(sps.csc_matrix(URM_idf).indptr)\n",
    "    # then normalize the values in each col\n",
    "    URM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "    URM_all = URM_idf  # use IDF features\n",
    "\n",
    "# #### Train/test split: ratings and user holdout\n",
    "\n",
    "seed = 0\n",
    "# ratings holdout\n",
    "# URM_train, URM_test_pred = train_test_holdout(URM_all, train_perc=0.8, seed=seed)\n",
    "# URM_valid=URM_test_pred\n",
    "# URM_test_known = None\n",
    "\n",
    "# user holdout\n",
    "# URM_train, URM_test_known, URM_test_pred = train_test_user_holdout(URM_all, user_perc=0.8, train_perc=0.8, seed=seed)\n",
    "\n",
    "# row holdout\n",
    "# URM_train, URM_test_pred = train_test_row_holdout(URM_all, userList_unique, train_sequential_df, train_perc=0.8, seed=seed, targetsListOrdered=targetsListOrdered, nnz_threshold=10)\n",
    "# URM_test_known = None\n",
    "\n",
    "# row holdout - validation\n",
    "# URM_train_val, URM_test_pred = train_test_row_holdout(URM_all, userList_unique, train_sequential_df, train_perc=0.8,\n",
    "#                                                       seed=seed, targetsListOrdered=targetsListOrdered,\n",
    "#                                                       nnz_threshold=10)\n",
    "# URM_train, URM_valid = train_test_holdout(URM_train_val, train_perc=0.7, seed=seed)\n",
    "# URM_test_known = None\n",
    "# URM_train, URM_valid_test_pred = train_test_row_holdout(URM_all, userList_unique, train_sequential_df,\n",
    "#                                                        train_perc=0.6,\n",
    "#                                                        seed=seed, targetsListOrdered=targetsListOrdered,\n",
    "#                                                        nnz_threshold=2)\n",
    "# URM_valid, URM_test_pred = train_test_row_holdout(URM_valid_test_pred, userList_unique, train_sequential_df,\n",
    "#                                                   train_perc=0.5,\n",
    "#                                                  seed=seed, targetsListOrdered=targetsListOrdered,\n",
    "#                                                  nnz_threshold=1)\n",
    "\n",
    "# URM_train = URM_train\n",
    "#URM_validation = URM_valid\n",
    "# URM_test = URM_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dump/dump_URM_train_rowholdout0802th2', 'rb') as dump_file:\n",
    "    URM_train = pickle.load(dump_file)\n",
    "with open('dump/dump_URM_test_rowholdout0802th2', 'rb') as dump_file:\n",
    "    URM_test = pickle.load(dump_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fitting the recommender\n",
    "\n",
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "This recommender is based on the following paper:\n",
    "\n",
    "_Rendle, S., Freudenthaler, C., & Schmidt-Thieme, L. (2010). Factorizing personalized Markov chains for next-basket recommendation. Proceedings of the 19th International Conference on World Wide Web - WWW ’10, 811_\n",
    "\n",
    "In short, FPMC factorizes a personalized order-1 transition tensor using Tensor Factorization with pairwise loss function akin to BPR (Bayesian Pairwise Ranking).\n",
    "\n",
    "<img src=\"images/fpmc.png\" width=\"200px\" />\n",
    "\n",
    "TF allows to impute values for the missing transitions between items for each user. For this reason, FPMC can be used for generating _personalized_ recommendations in session-aware recommenders as well.\n",
    "\n",
    "In this notebook, you will be able to change the number of latent factors and a few other learning hyper-parameters and see the impact on the recommendation quality.\n",
    "\n",
    "The class `FPMCRecommender` has the following initialization hyper-parameters:\n",
    "* `n_factor`: (optional) the number of latent factors\n",
    "* `learn_rate`: (optional) the learning rate\n",
    "* `regular`: (optional) the L2 regularization coefficient\n",
    "* `n_epoch`: (optional) the number of training epochs\n",
    "* `n_neg`: (optional) the number of negative samples used in BPR learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = FPMCRecommender(URM_train, train_sequential_df, targetsListOrdered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  4.39s/it]\n"
     ]
    }
   ],
   "source": [
    "recommender.fit(n_factor=16, n_epoch=2, learn_rate=0.01, regular=0.001, n_neg=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1662/5000 [01:09<02:45, 20.21it/s]"
     ]
    }
   ],
   "source": [
    "est_rat = recommender.compute_item_score(targetsListOrdered)\n",
    "print(est_rat)\n",
    "est_rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recommender' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0945739fc0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetsListOrdered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recommender' is not defined"
     ]
    }
   ],
   "source": [
    "recommendations = recommender.recommend(targetsListOrdered, cutoff=10)\n",
    "recommendations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixedMarkovChainRecommender: Saving model in file 'dump/dump_MixedMarkovChainRecommender_ord_1_1'\n",
      "{}: Saving complete\n",
      "MarkovChainRecommender: Saving model in file 'dump/dump_MixedMarkovChainRecommender_ord_1_1_MarkovChainRecommender_order_1'\n",
      "MarkovChainRecommender: Saving graph model in file 'dump/dump_MixedMarkovChainRecommender_ord_1_1_MarkovChainRecommender_order_1_G'\n",
      "{}: Saving complete\n"
     ]
    }
   ],
   "source": [
    "recommender.saveModel(\"dump/\", \"test_save_FPMCRecommender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_load = FPMCRecommender(URM_train, train_sequential_df, targetsListOrdered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
